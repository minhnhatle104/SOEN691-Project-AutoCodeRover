[
    {
        "role": "system",
        "content": "You are a software developer maintaining a large project.\nYou are working on an issue submitted to your project.\nThe issue contains a description marked between <issue> and </issue>.\nYour task is to invoke a few search API calls to gather buggy information, then write patches to solve the issues.\n"
    },
    {
        "role": "user",
        "content": "<issue>IndexError thrown with LogisticRegressionCV and refit=False\n#### Description\nThe following error is thrown when trying to estimate a regularization parameter via cross-validation, *without* refitting.\n#### Steps/Code to Reproduce\n```python\nimport sys\nimport sklearn\nfrom sklearn.linear_model import LogisticRegressionCV\nimport numpy as np\nnp.random.seed(29)\nX = np.random.normal(size=(1000, 3))\nbeta = np.random.normal(size=3)\nintercept = np.random.normal(size=None)\ny = np.sign(intercept + X @ beta)\nLogisticRegressionCV(\ncv=5,\nsolver='saga', # same error with 'liblinear'\ntol=1e-2,\nrefit=False).fit(X, y)\n```\n#### Expected Results\nNo error is thrown.\n#### Actual Results\n```\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\n<ipython-input-3-81609fd8d2ca> in <module>\n----> 1 LogisticRegressionCV(refit=False).fit(X, y)\n~/.pyenv/versions/3.6.7/envs/jupyter/lib/python3.6/site-packages/sklearn/linear_model/logistic.py in fit(self, X, y, sample_weight)\n2192                 else:\n2193                     w = np.mean([coefs_paths[:, i, best_indices[i], :]\n-> 2194                                  for i in range(len(folds))], axis=0)\n2195\n2196                 best_indices_C = best_indices % len(self.Cs_)\n~/.pyenv/versions/3.6.7/envs/jupyter/lib/python3.6/site-packages/sklearn/linear_model/logistic.py in <listcomp>(.0)\n2192                 else:\n2193                     w = np.mean([coefs_paths[:, i, best_indices[i], :]\n-> 2194                                  for i in range(len(folds))], axis=0)\n2195\n2196                 best_indices_C = best_indices % len(self.Cs_)\nIndexError: too many indices for array\n```\n#### Versions\n```\nSystem:\npython: 3.6.7 (default, May 13 2019, 16:14:45)  [GCC 4.2.1 Compatible Apple LLVM 10.0.1 (clang-1001.0.46.4)]\nexecutable: /Users/tsweetser/.pyenv/versions/3.6.7/envs/jupyter/bin/python\nmachine: Darwin-18.6.0-x86_64-i386-64bit\nBLAS:\nmacros: NO_ATLAS_INFO=3, HAVE_CBLAS=None\nlib_dirs:\ncblas_libs: cblas\nPython deps:\npip: 19.1.1\nsetuptools: 39.0.1\nsklearn: 0.21.2\nnumpy: 1.15.1\nscipy: 1.1.0\nCython: 0.29.6\npandas: 0.24.2\n```\n</issue>"
    },
    {
        "role": "user",
        "content": "Based on the files, classes, methods, code statements from the issue that related to the bug, you can use below search APIs to get more context of the project.\n        search_class(class_name: str): Search for a class in the codebase.\n        search_method_in_file(method_name: str, file_path: str): Search for a method in a given file.\n        search_method_in_class(method_name: str, class_name: str): Search for a method in a given class.\n        search_method(method_name: str): Search for a method in the entire codebase.\n        search_code(code_str: str): Search for a code snippet in the entire codebase.\n        search_code_in_file(code_str: str, file_path: str): Search for a code snippet in a given file file.\n        Note that you can use multiple search APIs in one round.\n        Now analyze the issue and select necessary APIs to get more context of the project, each API call must have concrete arguments as inputs.\n        "
    },
    {
        "role": "assistant",
        "content": "To analyze the issue effectively, we need to understand how the `fit` method is implemented in the `LogisticRegressionCV` class, especially the part where the error occurs. The error traceback points to operations involving `coefs_paths`, `best_indices`, and `folds`. Specifically, the error is related to an `IndexError` during the averaging of coefficient paths for different folds when `refit=False`. \n\nGiven this information, the following API calls will be useful to gather more context:\n\n1. Search for the `LogisticRegressionCV` class to understand its structure and related methods.\n2. Search for the `fit` method within the `LogisticRegressionCV` class to see how it is implemented.\n3. Search for the code snippet that is directly involved in the error, particularly the averaging operation over `coefs_paths`.\n\n```python\nsearch_class(\"LogisticRegressionCV\")\nsearch_method_in_class(\"fit\", \"LogisticRegressionCV\")\nsearch_code(\"coefs_paths[:, i, best_indices[i], :]\")\n```"
    },
    {
        "role": "user",
        "content": "Result of search_class(\"LogisticRegressionCV\"):\nFound 1 classes with name LogisticRegressionCV in the codebase.\nSearch result 1: <file>sklearn/linear_model/logistic.py</file> <class>LogisticRegressionCV</class> <code>class LogisticRegressionCV(LogisticRegression, BaseEstimator,\n                           LinearClassifierMixin):\n    def __init__(self, Cs=10, fit_intercept=True, cv=None, dual=False,\n                 penalty='l2', scoring=None, solver='lbfgs', tol=1e-4,\n                 max_iter=100, class_weight=None, n_jobs=None, verbose=0,\n                 refit=True, intercept_scaling=1., multi_class='auto',\n                 random_state=None, l1_ratios=None):\n    def fit(self, X, y, sample_weight=None):\n    def score(self, X, y, sample_weight=None):\n</code>\n\n\n\nResult of search_method_in_class(\"fit\", \"LogisticRegressionCV\"):\nFound 1 methods with name fit in class LogisticRegressionCV.\nSearch result 1: <file>sklearn/linear_model/logistic.py</file> <class>LogisticRegressionCV</class> <func>fit</func> <code>    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the model according to the given training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Training vector, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like, shape (n_samples,)\n            Target vector relative to X.\n\n        sample_weight : array-like, shape (n_samples,) optional\n            Array of weights that are assigned to individual samples.\n            If not provided, then each sample is given unit weight.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        solver = _check_solver(self.solver, self.penalty, self.dual)\n\n        if not isinstance(self.max_iter, numbers.Number) or self.max_iter < 0:\n            raise ValueError(\"Maximum number of iteration must be positive;\"\n                             \" got (max_iter=%r)\" % self.max_iter)\n        if not isinstance(self.tol, numbers.Number) or self.tol < 0:\n            raise ValueError(\"Tolerance for stopping criteria must be \"\n                             \"positive; got (tol=%r)\" % self.tol)\n        if self.penalty == 'elasticnet':\n            if self.l1_ratios is None or len(self.l1_ratios) == 0 or any(\n                    (not isinstance(l1_ratio, numbers.Number) or l1_ratio < 0\n                     or l1_ratio > 1) for l1_ratio in self.l1_ratios):\n                raise ValueError(\"l1_ratios must be a list of numbers between \"\n                                 \"0 and 1; got (l1_ratios=%r)\" %\n                                 self.l1_ratios)\n            l1_ratios_ = self.l1_ratios\n        else:\n            if self.l1_ratios is not None:\n                warnings.warn(\"l1_ratios parameter is only used when penalty \"\n                              \"is 'elasticnet'. Got (penalty={})\".format(\n                                  self.penalty))\n\n            l1_ratios_ = [None]\n\n        if self.penalty == 'none':\n            raise ValueError(\n                \"penalty='none' is not useful and not supported by \"\n                \"LogisticRegressionCV.\"\n            )\n\n        X, y = check_X_y(X, y, accept_sparse='csr', dtype=np.float64,\n                         order=\"C\",\n                         accept_large_sparse=solver != 'liblinear')\n        check_classification_targets(y)\n\n        class_weight = self.class_weight\n\n        # Encode for string labels\n        label_encoder = LabelEncoder().fit(y)\n        y = label_encoder.transform(y)\n        if isinstance(class_weight, dict):\n            class_weight = {label_encoder.transform([cls])[0]: v\n                            for cls, v in class_weight.items()}\n\n        # The original class labels\n        classes = self.classes_ = label_encoder.classes_\n        encoded_labels = label_encoder.transform(label_encoder.classes_)\n\n        multi_class = _check_multi_class(self.multi_class, solver,\n                                         len(classes))\n\n        if solver in ['sag', 'saga']:\n            max_squared_sum = row_norms(X, squared=True).max()\n        else:\n            max_squared_sum = None\n\n        # init cross-validation generator\n        cv = check_cv(self.cv, y, classifier=True)\n        folds = list(cv.split(X, y))\n\n        # Use the label encoded classes\n        n_classes = len(encoded_labels)\n\n        if n_classes < 2:\n            raise ValueError(\"This solver needs samples of at least 2 classes\"\n                             \" in the data, but the data contains only one\"\n                             \" class: %r\" % classes[0])\n\n        if n_classes == 2:\n            # OvR in case of binary problems is as good as fitting\n            # the higher label\n            n_classes = 1\n            encoded_labels = encoded_labels[1:]\n            classes = classes[1:]\n\n        # We need this hack to iterate only once over labels, in the case of\n        # multi_class = multinomial, without changing the value of the labels.\n        if multi_class == 'multinomial':\n            iter_encoded_labels = iter_classes = [None]\n        else:\n            iter_encoded_labels = encoded_labels\n            iter_classes = classes\n\n        # compute the class weights for the entire dataset y\n        if class_weight == \"balanced\":\n            class_weight = compute_class_weight(class_weight,\n                                                np.arange(len(self.classes_)),\n                                                y)\n            class_weight = dict(enumerate(class_weight))\n\n        path_func = delayed(_log_reg_scoring_path)\n\n        # The SAG solver releases the GIL so it's more efficient to use\n        # threads for this solver.\n        if self.solver in ['sag', 'saga']:\n            prefer = 'threads'\n        else:\n            prefer = 'processes'\n\n        fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n                               **_joblib_parallel_args(prefer=prefer))(\n            path_func(X, y, train, test, pos_class=label, Cs=self.Cs,\n                      fit_intercept=self.fit_intercept, penalty=self.penalty,\n                      dual=self.dual, solver=solver, tol=self.tol,\n                      max_iter=self.max_iter, verbose=self.verbose,\n                      class_weight=class_weight, scoring=self.scoring,\n                      multi_class=multi_class,\n                      intercept_scaling=self.intercept_scaling,\n                      random_state=self.random_state,\n                      max_squared_sum=max_squared_sum,\n                      sample_weight=sample_weight,\n                      l1_ratio=l1_ratio\n                      )\n            for label in iter_encoded_labels\n            for train, test in folds\n            for l1_ratio in l1_ratios_)\n\n        # _log_reg_scoring_path will output different shapes depending on the\n        # multi_class param, so we need to reshape the outputs accordingly.\n        # Cs is of shape (n_classes . n_folds . n_l1_ratios, n_Cs) and all the\n        # rows are equal, so we just take the first one.\n        # After reshaping,\n        # - scores is of shape (n_classes, n_folds, n_Cs . n_l1_ratios)\n        # - coefs_paths is of shape\n        #  (n_classes, n_folds, n_Cs . n_l1_ratios, n_features)\n        # - n_iter is of shape\n        #  (n_classes, n_folds, n_Cs . n_l1_ratios) or\n        #  (1, n_folds, n_Cs . n_l1_ratios)\n        coefs_paths, Cs, scores, n_iter_ = zip(*fold_coefs_)\n        self.Cs_ = Cs[0]\n        if multi_class == 'multinomial':\n            coefs_paths = np.reshape(\n                coefs_paths,\n                (len(folds),  len(l1_ratios_) * len(self.Cs_), n_classes, -1)\n            )\n            # equiv to coefs_paths = np.moveaxis(coefs_paths, (0, 1, 2, 3),\n            #                                                 (1, 2, 0, 3))\n            coefs_paths = np.swapaxes(coefs_paths, 0, 1)\n            coefs_paths = np.swapaxes(coefs_paths, 0, 2)\n            self.n_iter_ = np.reshape(\n                n_iter_,\n                (1, len(folds), len(self.Cs_) * len(l1_ratios_))\n            )\n            # repeat same scores across all classes\n            scores = np.tile(scores, (n_classes, 1, 1))\n        else:\n            coefs_paths = np.reshape(\n                coefs_paths,\n                (n_classes, len(folds), len(self.Cs_) * len(l1_ratios_),\n                 -1)\n            )\n            self.n_iter_ = np.reshape(\n                n_iter_,\n                (n_classes, len(folds), len(self.Cs_) * len(l1_ratios_))\n            )\n        scores = np.reshape(scores, (n_classes, len(folds), -1))\n        self.scores_ = dict(zip(classes, scores))\n        self.coefs_paths_ = dict(zip(classes, coefs_paths))\n\n        self.C_ = list()\n        self.l1_ratio_ = list()\n        self.coef_ = np.empty((n_classes, X.shape[1]))\n        self.intercept_ = np.zeros(n_classes)\n        for index, (cls, encoded_label) in enumerate(\n                zip(iter_classes, iter_encoded_labels)):\n\n            if multi_class == 'ovr':\n                scores = self.scores_[cls]\n                coefs_paths = self.coefs_paths_[cls]\n            else:\n                # For multinomial, all scores are the same across classes\n                scores = scores[0]\n                # coefs_paths will keep its original shape because\n                # logistic_regression_path expects it this way\n\n            if self.refit:\n                # best_index is between 0 and (n_Cs . n_l1_ratios - 1)\n                # for example, with n_cs=2 and n_l1_ratios=3\n                # the layout of scores is\n                # [c1, c2, c1, c2, c1, c2]\n                #   l1_1 ,  l1_2 ,  l1_3\n                best_index = scores.sum(axis=0).argmax()\n\n                best_index_C = best_index % len(self.Cs_)\n                C_ = self.Cs_[best_index_C]\n                self.C_.append(C_)\n\n                best_index_l1 = best_index // len(self.Cs_)\n                l1_ratio_ = l1_ratios_[best_index_l1]\n                self.l1_ratio_.append(l1_ratio_)\n\n                if multi_class == 'multinomial':\n                    coef_init = np.mean(coefs_paths[:, :, best_index, :],\n                                        axis=1)\n                else:\n                    coef_init = np.mean(coefs_paths[:, best_index, :], axis=0)\n\n                # Note that y is label encoded and hence pos_class must be\n                # the encoded label / None (for 'multinomial')\n                w, _, _ = _logistic_regression_path(\n                    X, y, pos_class=encoded_label, Cs=[C_], solver=solver,\n                    fit_intercept=self.fit_intercept, coef=coef_init,\n                    max_iter=self.max_iter, tol=self.tol,\n                    penalty=self.penalty,\n                    class_weight=class_weight,\n                    multi_class=multi_class,\n                    verbose=max(0, self.verbose - 1),\n                    random_state=self.random_state,\n                    check_input=False, max_squared_sum=max_squared_sum,\n                    sample_weight=sample_weight,\n                    l1_ratio=l1_ratio_)\n                w = w[0]\n\n            else:\n                # Take the best scores across every fold and the average of\n                # all coefficients corresponding to the best scores.\n                best_indices = np.argmax(scores, axis=1)\n                if self.multi_class == 'ovr':\n                    w = np.mean([coefs_paths[i, best_indices[i], :]\n                                 for i in range(len(folds))], axis=0)\n                else:\n                    w = np.mean([coefs_paths[:, i, best_indices[i], :]\n                                 for i in range(len(folds))], axis=0)\n\n                best_indices_C = best_indices % len(self.Cs_)\n                self.C_.append(np.mean(self.Cs_[best_indices_C]))\n\n                best_indices_l1 = best_indices // len(self.Cs_)\n                self.l1_ratio_.append(np.mean(l1_ratios_[best_indices_l1]))\n\n            if multi_class == 'multinomial':\n                self.C_ = np.tile(self.C_, n_classes)\n                self.l1_ratio_ = np.tile(self.l1_ratio_, n_classes)\n                self.coef_ = w[:, :X.shape[1]]\n                if self.fit_intercept:\n                    self.intercept_ = w[:, -1]\n            else:\n                self.coef_[index] = w[: X.shape[1]]\n                if self.fit_intercept:\n                    self.intercept_[index] = w[-1]\n\n        self.C_ = np.asarray(self.C_)\n        self.l1_ratio_ = np.asarray(self.l1_ratio_)\n        self.l1_ratios_ = np.asarray(l1_ratios_)\n        # if elasticnet was used, add the l1_ratios dimension to some\n        # attributes\n        if self.l1_ratios is not None:\n            for cls, coefs_path in self.coefs_paths_.items():\n                self.coefs_paths_[cls] = coefs_path.reshape(\n                    (len(folds), self.Cs_.size, self.l1_ratios_.size, -1))\n            for cls, score in self.scores_.items():\n                self.scores_[cls] = score.reshape(\n                    (len(folds), self.Cs_.size, self.l1_ratios_.size))\n            self.n_iter_ = self.n_iter_.reshape(\n                (-1, len(folds), self.Cs_.size, self.l1_ratios_.size))\n\n        return self\n</code>\n\nOther results are in these files:\n\n\nResult of search_code(\"coefs_paths[:, i, best_indices[i], :]\"):\nFound 1 snippets containing `coefs_paths[:, i, best_indices[i], :]` in the codebase.\nSearch result 1: <file>sklearn/linear_model/logistic.py</file> <class>LogisticRegressionCV</class> <func>fit</func> <code>\n                                 for i in range(len(folds))], axis=0)\n                else:\n                    w = np.mean([coefs_paths[:, i, best_indices[i], :]\n                                 for i in range(len(folds))], axis=0)\n\n                best_indices_C = best_indices % len(self.Cs_)</code>\n\n\n\n"
    },
    {
        "role": "user",
        "content": "Let's analyze collected context first"
    },
    {
        "role": "assistant",
        "content": "Based on the collected context from the `LogisticRegressionCV` class and its `fit` method, we can identify several key points relevant to the issue:\n\n1. **Class and Method Overview**: The `LogisticRegressionCV` class is designed to perform logistic regression with cross-validation to select the regularization parameter (`C`) and, optionally, the `l1_ratio` for Elastic-Net regularization. The `fit` method is responsible for fitting the model according to the given training data, handling both binary and multinomial logistic regression, and supporting different solvers.\n\n2. **Error Context**: The `IndexError` occurs during the averaging of coefficients (`coefs_paths`) for different folds when `refit=False`. Specifically, the error arises in the following code snippet:\n   ```python\n   w = np.mean([coefs_paths[:, i, best_indices[i], :]\n                for i in range(len(folds))], axis=0)\n   ```\n   This part of the code is executed when the model is not being refitted after cross-validation. It attempts to compute the average coefficients across folds based on the best scores' indices (`best_indices`).\n\n3. **Potential Cause**: The `IndexError: too many indices for array` suggests that there's a mismatch in the expected dimensions of the `coefs_paths` array or an incorrect indexing operation. Given that this error occurs when `refit=False`, it's likely related to how `coefs_paths` is structured or accessed in this specific scenario.\n\n4. **Refit Logic**: When `refit=True`, the model is refitted on the entire dataset using the best found parameters. The logic for handling `refit=False` involves selecting the best indices and then computing the average coefficients across folds for those indices. The error suggests that there might be an assumption about the array's shape that doesn't hold in all cases, especially when not refitting.\n\n### Analysis Steps:\nTo address the issue, we need to consider the following steps:\n- **Verify Array Dimensions**: Check the dimensions of `coefs_paths` and how they are expected to change based on the `multi_class` setting and the `refit` parameter. It's crucial to understand the expected shape in the context where `refit=False`.\n- **Indexing Operation**: Review the indexing operation `coefs_paths[:, i, best_indices[i], :]` to ensure it's valid for the array's dimensions in this context. There might be a need to adjust how indices are computed or accessed.\n- **Testing Scenarios**: It would be beneficial to test different configurations, including varying `multi_class` settings, to identify if the issue is more prevalent under specific conditions.\n\nGiven the information at hand, the next step would be to draft a patch that ensures the indexing operation aligns with the expected dimensions of `coefs_paths` in all scenarios, particularly when `refit=False`. This might involve adding checks or modifying how `best_indices` are used to access `coefs_paths`."
    },
    {
        "role": "user",
        "content": "Based on your analysis, answer below questions:  - do we need more context: construct search API calls to get more context of the project. (leave it empty if you don't need more context)  - where are bug locations: buggy files and methods. (leave it empty if you don't have enough information)"
    },
    {
        "role": "assistant",
        "content": "- **Do we need more context**: \n\n- **Where are bug locations**: \n  - **Buggy File**: `sklearn/linear_model/logistic.py`\n  - **Buggy Method**: `fit` method within the `LogisticRegressionCV` class. Specifically, the bug is located in the part of the method that handles the scenario when `refit=False`, involving the averaging of `coefs_paths` based on `best_indices`."
    },
    {
        "role": "user",
        "content": "Here are the code in buggy locations:\nFound 1 methods with name fit in class LogisticRegressionCV.\nSearch result 1: <file>sklearn/linear_model/logistic.py</file> <class>LogisticRegressionCV</class> <func>fit</func> <code>    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the model according to the given training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Training vector, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like, shape (n_samples,)\n            Target vector relative to X.\n\n        sample_weight : array-like, shape (n_samples,) optional\n            Array of weights that are assigned to individual samples.\n            If not provided, then each sample is given unit weight.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        solver = _check_solver(self.solver, self.penalty, self.dual)\n\n        if not isinstance(self.max_iter, numbers.Number) or self.max_iter < 0:\n            raise ValueError(\"Maximum number of iteration must be positive;\"\n                             \" got (max_iter=%r)\" % self.max_iter)\n        if not isinstance(self.tol, numbers.Number) or self.tol < 0:\n            raise ValueError(\"Tolerance for stopping criteria must be \"\n                             \"positive; got (tol=%r)\" % self.tol)\n        if self.penalty == 'elasticnet':\n            if self.l1_ratios is None or len(self.l1_ratios) == 0 or any(\n                    (not isinstance(l1_ratio, numbers.Number) or l1_ratio < 0\n                     or l1_ratio > 1) for l1_ratio in self.l1_ratios):\n                raise ValueError(\"l1_ratios must be a list of numbers between \"\n                                 \"0 and 1; got (l1_ratios=%r)\" %\n                                 self.l1_ratios)\n            l1_ratios_ = self.l1_ratios\n        else:\n            if self.l1_ratios is not None:\n                warnings.warn(\"l1_ratios parameter is only used when penalty \"\n                              \"is 'elasticnet'. Got (penalty={})\".format(\n                                  self.penalty))\n\n            l1_ratios_ = [None]\n\n        if self.penalty == 'none':\n            raise ValueError(\n                \"penalty='none' is not useful and not supported by \"\n                \"LogisticRegressionCV.\"\n            )\n\n        X, y = check_X_y(X, y, accept_sparse='csr', dtype=np.float64,\n                         order=\"C\",\n                         accept_large_sparse=solver != 'liblinear')\n        check_classification_targets(y)\n\n        class_weight = self.class_weight\n\n        # Encode for string labels\n        label_encoder = LabelEncoder().fit(y)\n        y = label_encoder.transform(y)\n        if isinstance(class_weight, dict):\n            class_weight = {label_encoder.transform([cls])[0]: v\n                            for cls, v in class_weight.items()}\n\n        # The original class labels\n        classes = self.classes_ = label_encoder.classes_\n        encoded_labels = label_encoder.transform(label_encoder.classes_)\n\n        multi_class = _check_multi_class(self.multi_class, solver,\n                                         len(classes))\n\n        if solver in ['sag', 'saga']:\n            max_squared_sum = row_norms(X, squared=True).max()\n        else:\n            max_squared_sum = None\n\n        # init cross-validation generator\n        cv = check_cv(self.cv, y, classifier=True)\n        folds = list(cv.split(X, y))\n\n        # Use the label encoded classes\n        n_classes = len(encoded_labels)\n\n        if n_classes < 2:\n            raise ValueError(\"This solver needs samples of at least 2 classes\"\n                             \" in the data, but the data contains only one\"\n                             \" class: %r\" % classes[0])\n\n        if n_classes == 2:\n            # OvR in case of binary problems is as good as fitting\n            # the higher label\n            n_classes = 1\n            encoded_labels = encoded_labels[1:]\n            classes = classes[1:]\n\n        # We need this hack to iterate only once over labels, in the case of\n        # multi_class = multinomial, without changing the value of the labels.\n        if multi_class == 'multinomial':\n            iter_encoded_labels = iter_classes = [None]\n        else:\n            iter_encoded_labels = encoded_labels\n            iter_classes = classes\n\n        # compute the class weights for the entire dataset y\n        if class_weight == \"balanced\":\n            class_weight = compute_class_weight(class_weight,\n                                                np.arange(len(self.classes_)),\n                                                y)\n            class_weight = dict(enumerate(class_weight))\n\n        path_func = delayed(_log_reg_scoring_path)\n\n        # The SAG solver releases the GIL so it's more efficient to use\n        # threads for this solver.\n        if self.solver in ['sag', 'saga']:\n            prefer = 'threads'\n        else:\n            prefer = 'processes'\n\n        fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n                               **_joblib_parallel_args(prefer=prefer))(\n            path_func(X, y, train, test, pos_class=label, Cs=self.Cs,\n                      fit_intercept=self.fit_intercept, penalty=self.penalty,\n                      dual=self.dual, solver=solver, tol=self.tol,\n                      max_iter=self.max_iter, verbose=self.verbose,\n                      class_weight=class_weight, scoring=self.scoring,\n                      multi_class=multi_class,\n                      intercept_scaling=self.intercept_scaling,\n                      random_state=self.random_state,\n                      max_squared_sum=max_squared_sum,\n                      sample_weight=sample_weight,\n                      l1_ratio=l1_ratio\n                      )\n            for label in iter_encoded_labels\n            for train, test in folds\n            for l1_ratio in l1_ratios_)\n\n        # _log_reg_scoring_path will output different shapes depending on the\n        # multi_class param, so we need to reshape the outputs accordingly.\n        # Cs is of shape (n_classes . n_folds . n_l1_ratios, n_Cs) and all the\n        # rows are equal, so we just take the first one.\n        # After reshaping,\n        # - scores is of shape (n_classes, n_folds, n_Cs . n_l1_ratios)\n        # - coefs_paths is of shape\n        #  (n_classes, n_folds, n_Cs . n_l1_ratios, n_features)\n        # - n_iter is of shape\n        #  (n_classes, n_folds, n_Cs . n_l1_ratios) or\n        #  (1, n_folds, n_Cs . n_l1_ratios)\n        coefs_paths, Cs, scores, n_iter_ = zip(*fold_coefs_)\n        self.Cs_ = Cs[0]\n        if multi_class == 'multinomial':\n            coefs_paths = np.reshape(\n                coefs_paths,\n                (len(folds),  len(l1_ratios_) * len(self.Cs_), n_classes, -1)\n            )\n            # equiv to coefs_paths = np.moveaxis(coefs_paths, (0, 1, 2, 3),\n            #                                                 (1, 2, 0, 3))\n            coefs_paths = np.swapaxes(coefs_paths, 0, 1)\n            coefs_paths = np.swapaxes(coefs_paths, 0, 2)\n            self.n_iter_ = np.reshape(\n                n_iter_,\n                (1, len(folds), len(self.Cs_) * len(l1_ratios_))\n            )\n            # repeat same scores across all classes\n            scores = np.tile(scores, (n_classes, 1, 1))\n        else:\n            coefs_paths = np.reshape(\n                coefs_paths,\n                (n_classes, len(folds), len(self.Cs_) * len(l1_ratios_),\n                 -1)\n            )\n            self.n_iter_ = np.reshape(\n                n_iter_,\n                (n_classes, len(folds), len(self.Cs_) * len(l1_ratios_))\n            )\n        scores = np.reshape(scores, (n_classes, len(folds), -1))\n        self.scores_ = dict(zip(classes, scores))\n        self.coefs_paths_ = dict(zip(classes, coefs_paths))\n\n        self.C_ = list()\n        self.l1_ratio_ = list()\n        self.coef_ = np.empty((n_classes, X.shape[1]))\n        self.intercept_ = np.zeros(n_classes)\n        for index, (cls, encoded_label) in enumerate(\n                zip(iter_classes, iter_encoded_labels)):\n\n            if multi_class == 'ovr':\n                scores = self.scores_[cls]\n                coefs_paths = self.coefs_paths_[cls]\n            else:\n                # For multinomial, all scores are the same across classes\n                scores = scores[0]\n                # coefs_paths will keep its original shape because\n                # logistic_regression_path expects it this way\n\n            if self.refit:\n                # best_index is between 0 and (n_Cs . n_l1_ratios - 1)\n                # for example, with n_cs=2 and n_l1_ratios=3\n                # the layout of scores is\n                # [c1, c2, c1, c2, c1, c2]\n                #   l1_1 ,  l1_2 ,  l1_3\n                best_index = scores.sum(axis=0).argmax()\n\n                best_index_C = best_index % len(self.Cs_)\n                C_ = self.Cs_[best_index_C]\n                self.C_.append(C_)\n\n                best_index_l1 = best_index // len(self.Cs_)\n                l1_ratio_ = l1_ratios_[best_index_l1]\n                self.l1_ratio_.append(l1_ratio_)\n\n                if multi_class == 'multinomial':\n                    coef_init = np.mean(coefs_paths[:, :, best_index, :],\n                                        axis=1)\n                else:\n                    coef_init = np.mean(coefs_paths[:, best_index, :], axis=0)\n\n                # Note that y is label encoded and hence pos_class must be\n                # the encoded label / None (for 'multinomial')\n                w, _, _ = _logistic_regression_path(\n                    X, y, pos_class=encoded_label, Cs=[C_], solver=solver,\n                    fit_intercept=self.fit_intercept, coef=coef_init,\n                    max_iter=self.max_iter, tol=self.tol,\n                    penalty=self.penalty,\n                    class_weight=class_weight,\n                    multi_class=multi_class,\n                    verbose=max(0, self.verbose - 1),\n                    random_state=self.random_state,\n                    check_input=False, max_squared_sum=max_squared_sum,\n                    sample_weight=sample_weight,\n                    l1_ratio=l1_ratio_)\n                w = w[0]\n\n            else:\n                # Take the best scores across every fold and the average of\n                # all coefficients corresponding to the best scores.\n                best_indices = np.argmax(scores, axis=1)\n                if self.multi_class == 'ovr':\n                    w = np.mean([coefs_paths[i, best_indices[i], :]\n                                 for i in range(len(folds))], axis=0)\n                else:\n                    w = np.mean([coefs_paths[:, i, best_indices[i], :]\n                                 for i in range(len(folds))], axis=0)\n\n                best_indices_C = best_indices % len(self.Cs_)\n                self.C_.append(np.mean(self.Cs_[best_indices_C]))\n\n                best_indices_l1 = best_indices // len(self.Cs_)\n                self.l1_ratio_.append(np.mean(l1_ratios_[best_indices_l1]))\n\n            if multi_class == 'multinomial':\n                self.C_ = np.tile(self.C_, n_classes)\n                self.l1_ratio_ = np.tile(self.l1_ratio_, n_classes)\n                self.coef_ = w[:, :X.shape[1]]\n                if self.fit_intercept:\n                    self.intercept_ = w[:, -1]\n            else:\n                self.coef_[index] = w[: X.shape[1]]\n                if self.fit_intercept:\n                    self.intercept_[index] = w[-1]\n\n        self.C_ = np.asarray(self.C_)\n        self.l1_ratio_ = np.asarray(self.l1_ratio_)\n        self.l1_ratios_ = np.asarray(l1_ratios_)\n        # if elasticnet was used, add the l1_ratios dimension to some\n        # attributes\n        if self.l1_ratios is not None:\n            for cls, coefs_path in self.coefs_paths_.items():\n                self.coefs_paths_[cls] = coefs_path.reshape(\n                    (len(folds), self.Cs_.size, self.l1_ratios_.size, -1))\n            for cls, score in self.scores_.items():\n                self.scores_[cls] = score.reshape(\n                    (len(folds), self.Cs_.size, self.l1_ratios_.size))\n            self.n_iter_ = self.n_iter_.reshape(\n                (-1, len(folds), self.Cs_.size, self.l1_ratios_.size))\n\n        return self\n</code>\n\nOther results are in these files:\n\n"
    }
]